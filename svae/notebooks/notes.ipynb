{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N(x|\\mu, \\Sigma)=\n",
    "$$\n",
    "\n",
    "A normalized coordinates matrix, values of which will be used as an input to the decoder ($28 \\times 28$ in the case of MNIST):\n",
    "\n",
    "$$\n",
    "I = \n",
    "\\begin{bmatrix}\n",
    "(-1, 1)^T  & \\dots  & (0, 1)^T  & \\dots  & (1, 1)^T  \\\\\n",
    "\\vdots     & \\ddots & \\vdots  & \\ddots & \\vdots      \\\\\n",
    "(-1, 0)^T  & \\dots  & (0, 0)^T  & \\dots  & (1, 0)^T  \\\\\n",
    "\\vdots     & \\ddots & \\vdots  & \\ddots & \\vdots      \\\\\n",
    "(-1, -1)^T & \\dots  & (0, -1)^T & \\dots  & (1, -1)^T \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A rotation matrix that will be used to transform the points above:\n",
    "\n",
    "$$\n",
    "R(\\theta) = \n",
    "\\begin{bmatrix}\n",
    "cos(\\theta) & -sin(\\theta) \\\\\n",
    "sin(\\theta) & cos(\\theta)  \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A translation matrix that will be used to transform the points above:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Delta x} = \n",
    "\\begin{bmatrix}\n",
    "\\Delta x_1 \\\\\n",
    "\\Delta x_2 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A transformed coordinate $\\mathbf{x}$ that will go into the encoder is generated as:\n",
    "\n",
    "$$\n",
    "\\mathbf{x} = I_{ij} R(\\theta) + \\mathbf{\\Delta x}\n",
    "$$\n",
    "\n",
    "From (Kingma and Welling, 2014), the KL divergence for the unconstrained latent variables $\\mathbf{z}$, and the translation variables are as follows:\n",
    "\n",
    "$$\n",
    "-D_{KL} \\left( q(\\mathbf{z} | \\mathbf{y}) || p(\\mathbf{z}) \\right) = \n",
    "\\frac{1}{2} \\sum_{j=1}^{J} {\\left( 1 + ln(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2 \\right)}\n",
    "\\\\\n",
    "-D_{KL} \\left( q(\\mathbf{\\Delta x} | \\mathbf{y}) || p(\\mathbf{\\Delta x}) \\right) = \n",
    "\\frac{1}{2} \\sum_{j=1}^{J} {\\left( 1 + ln(\\sigma_j^2) - \\mu_j^2 - \\sigma_j^2 \\right)}\n",
    "$$\n",
    "\n",
    "The function to be optimised during training (equation (4) from (Tristan Bepler et al., 2019)) is the sum of the reconstruction loss (```torch.nn.BCEWithLogitsLoss(reduction='sum')``` in PyTorch), and the total KL divergence. Note that the KL divergence for the rotation is different from those above (see equation (3) from (Tristan Bepler et al., 2019)).\n",
    "\n",
    "To generate the inputs for the decoder, the outputs from the encoder need to be reparameterized as follows:\n",
    "\n",
    "$$\n",
    "z_i = \\mu_{z_i} + \\sigma_{z_i} \\epsilon,\n",
    "$$\n",
    "\n",
    "where $ \\epsilon \\sim \\mathcal{N}(0, 1) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('.env': venv)",
   "language": "python",
   "name": "python36864bitenvvenv4ecfb0bb56ca430aae0d93058c896355"
  },
  "language_info": {
   "name": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
